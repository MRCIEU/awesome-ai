# awesome-ai

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
![Maintenance](https://img.shields.io/maintenance/yes/2019)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)

Awesome list for all things AI, ML and deep learning

![](images/AI-vs-ML-vs-Deep-Learning.png)

**Table of Contents**

- [awesome-ai](#awesome-ai)
    - [Concepts](#concepts)
        - [Neural Machine Translation - attention mechanisms](#neural-machine-translation---attention-mechanisms)
        - [Transformer](#transformer)
        - [Universal Transformers](#universal-transformers)
    - [node2vec](#node2vec)
    - [Topic: graph analytics](#topic-graph-analytics)
    - [Tooling](#tooling)
        - [general purpose](#general-purpose)
        - [probalistic inference](#probalistic-inference)
        - [graph](#graph)
    - [Tutorials](#tutorials)
    - [Research papers / blog articles](#research-papers--blog-articles)
        - [NLP: methods](#nlp-methods)
        - [NLP: applications](#nlp-applications)
        - [Transformer applications](#transformer-applications)
    - [Awesome lists](#awesome-lists)
        - [General](#general)
        - [Frameworks / ecosystems](#frameworks--ecosystems)
        - [Awesome papers](#awesome-papers)
        - [NLP](#nlp)
        - [Graph theory](#graph-theory)
        - [Knowledge graph](#knowledge-graph)
        - [Notebooks](#notebooks)

## Concepts

### Neural Machine Translation - attention mechanisms

Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)

https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/

### Transformer

A model that uses attention to boost the speed with which these models can be trained

http://jalammar.github.io/illustrated-transformer/

### Universal Transformers

The Universal Transformer is an extension to the Transformer models which combines the parallelizability and global receptive field of the Transformer model with the recurrent inductive bias of RNNs, which seems to be better suited to a range of algorithmic and natural language understanding sequence-to-sequence problems.

http://mostafadehghani.com/2019/05/05/universal-transformers/

## node2vec

An algorithmic framework for learning useful representation from highly structured objects such as graphs.

http://sujitpal.blogspot.com/2019/05/node2vec-graph-embeddings-for-neurips.html?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A+SalmonRun+%28Salmon+Run%29

## Topic: graph analytics

- [Benchmark of popular graph/network packages in python/r](https://www.timlrx.com/2019/05/05/benchmark-of-popular-graph-network-packages/)

## Tooling

### general purpose

### probalistic inference

### graph

- [dgl: Python package built to ease deep learning on graph, on top of existing DL frameworks.](https://github.com/dmlc/dgl)
- [cdlib: Community Discovery Library - (for networkx and igraph)](https://github.com/GiulioRossetti/cdlib)
- [Embedding-Vis](https://github.com/meltzerpete/Embedding-Vis)
- [OpenNE](https://github.com/thunlp/OpenNE)

### web resources

- [clocate.com; search conferences](https://www.clocate.com/)

## Tutorials

- [fastai: practical deep learning for coders](https://course.fast.ai/)
- [practicalAI](https://github.com/GokuMohandas/practicalAI)
- [Machine-Learning-Tutorials](https://github.com/ujjwalkarn/Machine-Learning-Tutorials)

## Research papers / blog articles

### NLP: methods

- [Parikh, A.P., Täckström, O., Das, D. and Uszkoreit, J., 2016. A decomposable attention model for natural language inference. arXiv preprint arXiv:1606.01933.](https://arxiv.org/abs/1606.01933)
- [Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. and Polosukhin, I., 2017. Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008).](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)
- [Devlin, J., Chang, M.W., Lee, K. and Toutanova, K., 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.](https://arxiv.org/pdf/1810.04805.pdf)
- [The illustrated GPT-2 (Visulaizing Transformer Language Models)](https://jalammar.github.io/illustrated-gpt2/)
- [Transformers from scratch](http://www.peterbloem.nl/blog/transformers)
- [Searching for Answer Candidate Passages with Solr and Anserini](http://sujitpal.blogspot.com/2019/09/searching-for-answer-candidate-passages.html)
- [Subramanian et al., 2019. On extractive and abstractive neural document summarization with transformer language models](https://arxiv.org/abs/1909.03186)

### NLP: applications

- [Wang et al., 2018. Ontology Alignment in the Biomedical Domain Using Entity Definitions and Context](https://arxiv.org/pdf/1806.07976.pdf)
- [Wang et al., 2019., Extracting evidence of supplement-drug interactions from literature](https://arxiv.org/pdf/1909.08135.pdf)

### Transformer applications

- [Bert2Tag: BERT key phrase tagging](https://github.com/thunlp/Bert2Tag)

## Awesome lists

### General

- [awesome-deep-learning](https://github.com/ChristosChristofidis/awesome-deep-learning)
- [awesome-deep-learning-resources](https://github.com/guillaume-chevalier/Awesome-Deep-Learning-Resources)

### Frameworks / ecosystems

- [awesome-pytorch-list](https://github.com/bharathgs/Awesome-pytorch-list)
- [the-incredible-pytorch](https://github.com/ritchieng/the-incredible-pytorch)
- [awesome-ternsorflow](https://github.com/jtoy/awesome-tensorflow)
- [keras-resources](https://github.com/fchollet/keras-resources)
- [DataSciencePython](https://github.com/ujjwalkarn/DataSciencePython)
- [DataScienceR](https://github.com/ujjwalkarn/DataScienceR)

### Awesome papers

- [awesome-deep-learning-papers](https://github.com/terryum/awesome-deep-learning-papers)
- [awesome-decision-tree-papers](https://github.com/benedekrozemberczki/awesome-decision-tree-papers)
- [awesome-gradient-boosting-papers](https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers)
- [must read papers on graph neural network](https://github.com/thunlp/GNNPapers)
- [deep learning biology](https://github.com/hussius/deeplearning-biology)
- [awesome-deepbio](https://github.com/gokceneraslan/awesome-deepbio)

### NLP

- [nlp-roadmap](https://github.com/graykode/nlp-roadmap)
- [NLP-progress](https://github.com/sebastianruder/NLP-progress)
- [awesome-nlp](https://github.com/keon/awesome-nlp)
- [awesome-bert](https://github.com/Jiakui/awesome-bert)
- [awesome-word2vec](https://github.com/MaxwellRebo/awesome-2vec)
- [awesome-embedding-models](https://github.com/Hironsan/awesome-embedding-models)
- [NCBI BioNLP Research Group (PI: Zhiyong Lu)](https://github.com/ncbi-nlp)
- [awesome-bert](https://github.com/Jiakui/awesome-bert)
- [thunlp](https://github.com/thunlp) awesome paper repos
  - [neural relation extraction (NRE)](https://github.com/thunlp/NREPapers)
  - [machine reading comprehension](https://github.com/thunlp/RCPapers)
  - [network representation learning (NRL) / network embedding (NE)](https://github.com/thunlp/NRLPapers)
  - [graph neural networks (GNN)](https://github.com/thunlp/GNNPapers)
  - [knowledge representation learning (KRL) / knowledge embedding (KE)](https://github.com/thunlp/KRLPapers)
- [the super duper NLP repo](https://notebooks.quantumstat.com/)

### Graph theory

- [awesome-community-detection](https://github.com/benedekrozemberczki/awesome-community-detection)
- [awesome-graph-classification](https://github.com/benedekrozemberczki/awesome-graph-classification)
- [awesome-network-analysis](https://github.com/briatte/awesome-network-analysis)
- [awesome-network-embedding](https://github.com/chihming/awesome-network-embedding)
- [graph-based-deep-learning-literature](https://github.com/naganandy/graph-based-deep-learning-literature)
- [LiteratureDL4Graph](https://github.com/DeepGraphLearning/LiteratureDL4Graph)

### Knowledge graph

- [shaoxiongji/awesome-knowledge-graph](https://github.com/shaoxiongji/awesome-knowledge-graph)
- [totogo/awesome-knowledge-graph](https://github.com/totogo/awesome-knowledge-graph)
- [BrambleXu/knowledge-graph-learning](https://github.com/BrambleXu/knowledge-graph-learning)
- [husthuke/awesome-knowledge-graph (in Chinese)](https://github.com/husthuke/awesome-knowledge-graph)

### Notebooks

- [NLP graphs](https://github.com/sujitpal/nlp-graph-examples)
